---
title: "Assignment 6"
author: "Enock Soko"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)

library(tidyverse)
library(brms)
library(tidybayes)
library(kableExtra)
library(emmeans)
library(naniar)

```

# Introduction

In this report I will analyse group presentation marks assessed by multiple lecturers with incomplete overlap in which presentations they assessed. My goal is to produce fair marks for each group while accounting for assessor biases.

# Question 1: Residual Variability

The residual (error) variability in this problem comes from several sources:

*Assessor biases*: Each lecturer may have their own tendencies to mark higher or lower on average (fixed effects) and may vary in their strictness (random effects).

*Presentation variability*: Even the same group might perform differently on different occasions due to factors like nerves, technical issues, or time of day.

*Rubric interpretation*: Different assessors may interpret the rubric criteria slightly differently.

*Group dynamics*: The specific combination of assessors and groups may create unique interactions not accounted for in the model.

# Question 2: Average Assessor Mark Assumptions

If all assessors viewed all students and were equally neutral then the current assumptions would likely be sufficient for the average mark to be correct on average

However these would be additional assuptions needed:

-   All assessors have equal expertise in evaluating the presentations

-   The rubric is applied consistently across all presentations

-   No systematic patterns in missing data (missing completely at random)

-   No interaction effects between assessors and groups

# Question 3: Summary of data properties

## Loading Data and Initial Exploration

```{r}
data <- read_delim("BayesAssignment6of2025.csv", delim = ";")

data %>% head() %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = FALSE)
```

### Missing Data Patterns

```{r}

missing_summary <- data %>% 
  summarise(across(everything(), ~sum(is.na(.)))) %>% 
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing Count")
missing_summary %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = FALSE)

missing_pattern <- data %>% 
  select(starts_with("Lecturer")) %>% 
  summarise(across(everything(), ~sum(is.na(.))))
missing_pattern %>% kable(caption = "Missing Lecturer Assessments") %>% 
  kable_styling(bootstrap_options = "striped", full_width = FALSE)
```

The dataset contains the following:

-   13 student groups (Group1-Group13)

-   9 potential assessors (LecturerA-LecturerI)

-   4 previous performance measures (Proposal, Literature, Quiz, Interview)

-   Marks range from 52 to 96

When it comes Missingness patterns we observe the following:

-   Not all lecturers assessed all groups (sparse design)

-   Some groups have more assessments than others

-   Previous performance measures are complete except for Group5 missing Interview

# Question 4: Transforming our data to Long Format


```{r}

long_data <- data %>% 
  pivot_longer(
    cols = starts_with("Lecturer"),
    names_to = "Lecturer",
    values_to = "Mark",
    values_drop_na = TRUE
  ) %>% 
  mutate(Group = factor(Group), Lecturer = factor(Lecturer)) %>% 
  select(Group, Lecturer, Mark, Proposal, Literature, Quiz, Interview)
long_data %>% head() %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = FALSE)

missing_analysis <- long_data %>%
  group_by(Group) %>%
  summarise(n_assessments = n(), missing_lecturers = 9 - n_distinct(Lecturer), .groups = "drop")
missing_analysis %>% kable(caption = "Assessments per Group") %>% 
  kable_styling(bootstrap_options = "striped", full_width = FALSE)


```

# Question 5: Fixed vs Random Effects

**Fixed Effects**:

*Group*: We want explicit estimates for each group's performance

*Previous marks* (Proposal, Literature, Quiz, Interview): These are known covariates that should affect the current mark

**Random Effects**:

*Lecturer*: We expect lecturers to have random biases (some more generous, some stricter)

*Group-Lecturer interaction*: Allows for the possibility that certain lecturers grade certain groups differently

**Thinking behind my conclusions**:

Groups are of direct interest (fixed)

Lecturers are a sample from a larger population of potential assessors (random)

Previous marks are objective measures that should influence current marks (fixed)


# Question 6: Model Fitting

I am using the default prior.

```{r, warning=FALSE}

model <- brm(
  Mark ~ Group + scale(Proposal) + scale(Literature) + scale(Quiz) + scale(Interview) + (1 | Lecturer),
  data = long_data,
  chains = 4,
  iter = 4000,
  cores = 4,
  seed = 123,
  control = list(adapt_delta = 0.95)
)
summary(model)

```


# Question 7: Group Mark Estimates


```{r, warning=FALSE}

group_estimates <- emmeans(model, ~Group) %>% 
  as_tibble() %>% 
  select(Group, estimate = emmean, lower.HPD, upper.HPD) %>% 
  mutate(across(where(is.numeric), round, 2))

newdata <- distinct(long_data, Group, Proposal, Literature, Quiz, Interview) %>% 
  mutate(across(Proposal:Interview, ~mean(., na.rm = TRUE)))

pred_intervals <- posterior_predict(model, newdata = newdata, re_formula = NA) %>% 
  apply(2, quantile, probs = c(0.025, 0.975)) %>% 
  t() %>% 
  as_tibble() %>% 
  rename(pred_lower = `2.5%`, pred_upper = `97.5%`) %>% 
  mutate(Group = unique(long_data$Group))

final_estimates <- left_join(group_estimates, pred_intervals, by = "Group")
final_estimates %>% kable(digits = 2, caption = "Group Estimates and Intervals") %>% 
  kable_styling(bootstrap_options = "striped", full_width = FALSE)

```

# Question 8: Assessor Biases

The table below presents the estimated biases for each lecturer, derived from the random effects of the Bayesian mixed-effects model. A negative bias indicates a tendency to mark lower than the average, while a positive bias suggests a tendency to mark higher. The lecturer with the smallest absolute bias is considered the least biased.

```{r, warning=FALSE}

lecturer_effects <- ranef(model)$Lecturer %>%
  as_tibble(rownames = "Lecturer") %>%
  select(Lecturer, Bias = Estimate.Intercept, SE = Est.Error.Intercept) %>%
  arrange(Bias)

lecturer_effects %>%
  kable(digits = 2, caption = "Estimated Biases for Each Lecturer") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE)

least_biased <- lecturer_effects %>%
  mutate(abs_bias = abs(Bias)) %>%
  filter(abs_bias == min(abs_bias))

cat("The least biased lecturer is", least_biased$Lecturer, 
    "with an estimated bias of", round(least_biased$Bias, 2), 
    "(SE =", round(least_biased$SE, 2), ")\n")
```


# Question 9: Incorporating Previous Marks

```{r,warning=FALSE}

group_prev <- long_data %>%
  group_by(Group) %>%
  summarise(
    prev_perf = mean(c(Proposal, Literature, Quiz, Interview), na.rm = TRUE),
    prev_sd = sd(c(Proposal, Literature, Quiz, Interview), na.rm = TRUE)
  )

informed_priors <- c(
  prior(normal(70, 10), class = "Intercept"),  
  prior(normal(0, 5), class = "b", coef = paste0("Group", levels(long_data$Group))),
  prior(normal(0, 2), class = "sd"),
  prior(normal(0.5, 0.2), class = "b", coef = "scaleProposal"),
  prior(normal(0.5, 0.2), class = "b", coef = "scaleLiterature"),
  prior(normal(0.5, 0.2), class = "b", coef = "scaleQuiz"),
  prior(normal(0.5, 0.2), class = "b", coef = "scaleInterview")
)

informed_model <- update(model, prior = informed_priors)

model_comparison <- loo_compare(loo(model), loo(informed_model))
print(model_comparison)

# The informed priors help regularize estimates, particularly for groups with fewer assessments

```


# Question 10: Differentiating Individual Performance

**Practical strategy for individual differentiation**:

*Peer assessment*: Have group members confidentially rate each other's contributions

*Individual components*: Include some individually-assessed portions (e.g., Q&A session)

*Assessor training*: Train assessors to look for and record individual contributions

*Rubric refinement*: Include specific criteria for individual performance within group work

*Self-assessment*: Have students reflect on and justify their own contributions


**To combat assessor laziness bias**:

- Make individual assessment mandatory in the rubric

- Provide clear guidelines on what constitutes individual contribution

- Use simple rating scales for quick assessment

- Randomly verify some assessments for quality control

